---
title: "An Introduction to Probability using Simulation"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    number_sections: TRUE
    toc: TRUE
    toc_depth: 2
    toc_float:
      smooth_scroll: FALSE
  pdf_document: default
---

```{r knit_opts, include = FALSE}
rm(list = ls())

# library(tidyverse)
# library(scales)
# library(purrr)
# library(cowplot)

import::from(tibble      ,as_tibble, tibble, tribble, add_column, glimpse)
import::from(magrittr    ,"%>%", set_colnames)
import::from(purrr       ,map, map_dbl, rerun)
import::from(dplyr       ,mutate, filter, select, group_by, left_join, inner_join
                         ,summarise, summarise_at, vars, distinct, sample_n
                         ,pull, if_else, count, bind_rows, n, arrange, ungroup)
import::from(tidyr       ,gather, spread, nest, unnest)
import::from(ggplot2     ,ggplot, aes, xlab, ylab, ggtitle, geom_line, geom_point
                         ,geom_histogram, geom_boxplot, geom_bar, geom_col
                         ,scale_x_continuous, scale_y_continuous, scale_x_log10
                         ,expand_limits, theme, element_text, facet_grid
                         ,facet_wrap, theme_set, theme)
import::from(scales      ,comma)
import::from(matrixStats ,rowSds)



options(width = 80L
       ,warn  = 1)

knitr::opts_chunk$set(tidy  = FALSE
                     ,cache = FALSE
                     ,warning = FALSE
                     ,message = FALSE
                     ,fig.height =  7
                     ,fig.width  = 11)


set.seed(42)
```


# Law of Large Numbers


$$
\lim_{n \to \infty} \frac{1}{N} \sum f(X_n) \, p(X_n) \to E[f(X)]
$$

$$
f(x) = x
$$

This function gives us expectations for the mean.

```{r lln_norm_identity_data, echo=TRUE}
N <- 2^(0:25)

x_vals <- rnorm(max(N), 0, 1)

y_vals <- map_dbl(N, function(i) x_vals[1:i] %>% mean())

ggplot() +
    geom_line(aes(x = N, y = y_vals)) +
    scale_x_log10(labels = comma) +
    xlab("Sample Size") +
    ylab("Average Value")
```

$$
f(X) = x^2
$$

This function gives us expectations for the variance.

```{r lln_norm_xsq_data, echo=TRUE}
y_vals <- map_dbl(N, function(i) { (x_vals[1:i])^2 %>% mean() }) 

ggplot() +
    geom_line(aes(x = N, y = y_vals)) +
    scale_x_log10(labels = comma) +
    xlab("Sample Size") +
    ylab("Average Value")
```

```{r create_calc_function, echo=TRUE}
calc_expect <- function(size, x_vals, func) {
    x_vals[1:size] %>% func() %>% mean()
}

y_id_vals <- map_dbl(N, calc_expect, x_vals = x_vals, func = function(x) x)
y_sq_vals <- map_dbl(N, calc_expect, x_vals = x_vals, func = function(x) x*x)


n_iter <- 10

sim_id_tbl <- rerun(n_iter
                   ,tibble(size = N
                          ,vals = map_dbl(N
                                         ,calc_expect
                                         ,x_vals = rnorm(max(N), 0, 1)
                                         ,func = function(x) x))) %>%
    bind_rows(.id = 'sim_id')

ggplot(sim_id_tbl) +
    geom_line(aes(x = size, y = vals, group = sim_id)) +
    scale_x_log10(labels = comma) +
    xlab('Sample Size') +
    ylab('Expectation')


sim_sq_tbl <- rerun(n_iter
                   ,tibble(size = N
                          ,vals = map_dbl(N
                                         ,calc_expect
                                         ,x_vals = rnorm(max(N), 0, 1)
                                         ,func = function(x) x*x))) %>%
    bind_rows(.id = 'sim_id')

ggplot(sim_sq_tbl) +
    geom_line(aes(x = size, y = vals, group = sim_id)) +
    scale_x_log10(labels = comma) +
    xlab('Sample Size') +
    ylab('Expectation')
```

## Uniform Distribution

```{r uniform_expectation, echo=TRUE}
sim_id_tbl <- rerun(n_iter
                   ,tibble(size = N
                          ,vals = map_dbl(N
                                         ,calc_expect
                                         ,x_vals = runif(max(N), 0, 1)
                                         ,func = function(x) x))) %>%
    bind_rows(.id = 'sim_id')

ggplot(sim_id_tbl) +
    geom_line(aes(x = size, y = vals, group = sim_id)) +
    scale_x_log10(labels = comma) +
    xlab('Sample Size') +
    ylab('Expectation')


sim_sq_tbl <- rerun(n_iter
                   ,tibble(size = N
                          ,vals = map_dbl(N
                                         ,calc_expect
                                         ,x_vals = runif(max(N), 0, 1)
                                         ,func = function(x) x*x))) %>%
    bind_rows(.id = 'sim_id')

ggplot(sim_sq_tbl) +
    geom_line(aes(x = size, y = vals, group = sim_id)) +
    scale_x_log10(labels = comma) +
    xlab('Sample Size') +
    ylab('Expectation')
```

## Cauchy Distribution

```{r cauchy_expectation, echo=TRUE}
sim_id_tbl <- rerun(n_iter
   ,tibble(size = N
          ,vals = map_dbl(N
                         ,calc_expect
                         ,x_vals      = rcauchy(max(N), 0, 1)
                         ,func        = function(x) x))) %>%
    bind_rows(.id = 'sim_id')

ggplot(sim_id_tbl) +
    geom_line(aes(x = size, y = vals, group = sim_id)) +
    scale_x_log10(labels = comma) +
    xlab('Sample Size') +
    ylab('Expectation')


sim_sq_tbl <- rerun(n_iter
    ,tibble(size = N
           ,vals = map_dbl(N
                          ,calc_expect
                          ,x_vals      = rcauchy(max(N), 0, 1)
                          ,func        = function(x) x*x))) %>%
    bind_rows(.id = 'sim_id')

ggplot(sim_sq_tbl) +
    geom_line(aes(x = size, y = vals, group = sim_id)) +
    scale_x_log10(labels = comma) +
    xlab('Sample Size') +
    ylab('Expectation')
```

# Expected Value

## 1D6

```{r roll_d6, echo=TRUE}
n_roll <- 100000

onedice <- sample(1:6, n_roll, replace = TRUE)

ggplot(tibble(dice = onedice)) +
    geom_bar(aes(x = dice)) +
    scale_y_continuous(labels = comma) +
    xlab("Dice Roll") +
    ylab("Count")

mean(onedice)
```

## 2D6

```{r roll_2d6, echo=TRUE}
twodice_indiv <- matrix(sample(1:6, 2 * n_roll, replace = TRUE), ncol = 2)
twodice <- rowSums(twodice_indiv)

ggplot(tibble(dice = twodice)) +
    geom_bar(aes(x = dice)) +
    scale_y_continuous(labels = comma) +
    xlab("Dice Roll") +
    ylab("Count")

mean(twodice)
```

## 3D6

```{r roll_3d6, echo=TRUE}
threedice_indiv <- matrix(sample(1:6, 3 * n_roll, replace = TRUE), ncol = 3)
threedice <- rowSums(threedice_indiv)

ggplot(tibble(dice = threedice)) +
    geom_bar(aes(x = dice)) +
    scale_y_continuous(labels = comma) +
    xlab("Dice Roll") +
    ylab("Count")

mean(threedice)
```

## 4D6 - Drop Lowest

```{r roll_4d6_droplowest, echo=TRUE}
drop_lowest <- function(x) x %>% sort(decreasing = TRUE) %>% .[1:3] %>% sum()

topthree_indiv <- matrix(sample(1:6, 4 * n_roll, replace = TRUE), ncol = 4)
topthree <- apply(topthree_indiv, 1, drop_lowest)

ggplot(tibble(dice = topthree)) +
    geom_bar(aes(x = dice)) +
    scale_y_continuous(labels = comma) +
    xlab("Dice Roll") +
    ylab("Count")

mean(topthree)
```

## 5D6 - Drop Extremes

What if we roll 5d6 and drop both the highest and lowest result - how does
this compare to the 3d6 rolls?

```{r roll_5d6_dropextreme, echo=TRUE}
drop_extreme <- function(x) x %>% sort() %>% .[2:4] %>% sum()

five_dice <- matrix(sample(1:6, 5 * n_roll, replace = TRUE), ncol = 5)
fivethree <- apply(five_dice, 1, drop_extreme)

ggplot(tibble(dice = fivethree, orig = threedice)) +
    geom_bar(aes(x = dice)) +
    geom_bar(aes(x = threedice), alpha = 0.25, fill = 'red') +
    scale_y_continuous(labels = comma) +
    xlab("Dice Roll") +
    ylab("Count") +
    ggtitle("Comparison Histograms for 3D6 and 5D6 Drop Extremes")

mean(fivethree)
```



# Bayes Rule

A disease affects 1 in 1,000 people. A test for the disease exists, and gives
a positive result 99% of the time when performed on a sick patient. If the
person is not sick, it has a false alarm rate of 5%.

Given a person tests positive, what is the probability of the person being
sick?

```{r bayes_rule_basic, echo=TRUE}
n_sim <- 1000000

base_rate <- 0.001
true_rate <- 0.99
fa_rate   <- 0.05

data_tbl <- tibble(id = 1:n_sim) %>%
    mutate(sick_person = sample(c(TRUE, FALSE)
                               ,n_sim
                               ,prob = c(base_rate, 1 - base_rate)
                               ,replace = TRUE))

sick_tbl    <- data_tbl %>% filter(sick_person == TRUE)
notsick_tbl <- data_tbl %>% filter(sick_person == FALSE)

sick_tbl <- sick_tbl %>%
    mutate(test_result = sample(c(TRUE, FALSE)
                               ,n()
                               ,prob = c(true_rate, 1 - true_rate)
                               ,replace = TRUE))

notsick_tbl <- notsick_tbl %>%
    mutate(test_result = sample(c(TRUE, FALSE)
                               ,n()
                               ,prob = c(fa_rate, 1 - fa_rate)
                               ,replace = TRUE))


data_tbl <- list(sick_tbl, notsick_tbl) %>%
    bind_rows() %>%
    arrange(id)
```

* How many people are sick?
* How many people test positive?
* Of the people with a positive result, how many are actually sick?

```{r check_proportions, echo=TRUE}
data_tbl %>% filter(sick_person == TRUE) %>% nrow()

data_tbl %>% filter(test_result == TRUE) %>% nrow()

data_tbl %>%
    filter(test_result == TRUE) %>%
    summarise(n_sick = sum(sick_person), sick_prop = n_sick / n())
```

How does this depend on the false alarm rate?

```{r create_test_result_data, echo = TRUE}
create_medtest_data <- function(n_sim = 1e6, base_rate = 0.001, true_rate = 0.99, fa_rate = 0.05) {
    data_tbl <- tibble(id = 1:n_sim) %>%
    mutate(sick_person = sample(c(TRUE, FALSE)
                               ,n_sim
                               ,prob = c(base_rate, 1 - base_rate)
                               ,replace = TRUE))

    sick_tbl    <- data_tbl %>% filter(sick_person == TRUE)
    notsick_tbl <- data_tbl %>% filter(sick_person == FALSE)

    sick_tbl <- sick_tbl %>%
        mutate(test_result = sample(c(TRUE, FALSE)
                                   ,n()
                                   ,prob = c(true_rate, 1 - true_rate)
                                   ,replace = TRUE))

    notsick_tbl <- notsick_tbl %>%
        mutate(test_result = sample(c(TRUE, FALSE)
                                   ,n()
                                   ,prob = c(fa_rate, 1 - fa_rate)
                                   ,replace = TRUE))


    data_tbl <- list(sick_tbl, notsick_tbl) %>%
        bind_rows() %>%
        arrange(id)

    return(data_tbl)
}

fa_vals <- seq(0.001, 0.10, by = 0.001)

sick_prop <- map_dbl(fa_vals, function(fa) create_medtest_data(10000
                                                              ,base_rate = 0.001
                                                              ,true_rate = 0.99
                                                              ,fa_rate   = fa) %>%
            filter(test_result == TRUE) %>%
            summarise(sick_prop = sum(sick_person) / n()) %>%
            pull(sick_prop)
        )

ggplot() +
    geom_line(aes(x = fa_vals, y = sick_prop)) +
    expand_limits(y = 0) +
    xlab('False Alarm Rate') +
    ylab('Proportion of True Positives')
```

What if we have multiple tests?

```{r bayes_rule_multiple_tests, echo=TRUE}
create_two_medtest_data <- function(n_sim = 1e6, base_rate = 0.001, true_rate = 0.99, fa_rate = 0.05) {
    data_tbl <- tibble(id = 1:n_sim) %>%
    mutate(sick_person = sample(c(TRUE, FALSE)
                               ,n_sim
                               ,prob = c(base_rate, 1 - base_rate)
                               ,replace = TRUE))

    sick_tbl    <- data_tbl %>% filter(sick_person == TRUE)
    notsick_tbl <- data_tbl %>% filter(sick_person == FALSE)

    sick_tbl <- sick_tbl %>%
        mutate(test_result_1 = sample(c(TRUE, FALSE)
                                     ,n()
                                     ,prob = c(true_rate, 1 - true_rate)
                                     ,replace = TRUE)
              ,test_result_2 = sample(c(TRUE, FALSE)
                                     ,n()
                                     ,prob = c(true_rate, 1 - true_rate)
                                     ,replace = TRUE)
               )

    notsick_tbl <- notsick_tbl %>%
        mutate(test_result_1 = sample(c(TRUE, FALSE)
                                     ,n()
                                     ,prob = c(fa_rate, 1 - fa_rate)
                                     ,replace = TRUE)
              ,test_result_2 = sample(c(TRUE, FALSE)
                                     ,n()
                                     ,prob = c(fa_rate, 1 - fa_rate)
                                     ,replace = TRUE)
               )


    data_tbl <- list(sick_tbl, notsick_tbl) %>%
        bind_rows() %>%
        arrange(id)

    return(data_tbl)
}

fa_vals <- seq(0.001, 0.10, by = 0.001)

sick_2_prop <- map_dbl(fa_vals
                      ,function(fa) create_two_medtest_data(10000
                                                           ,base_rate = 0.001
                                                           ,true_rate = 0.99
                                                           ,fa_rate = fa) %>%
        filter(test_result_1 == TRUE, test_result_2 == TRUE) %>%
        summarise(sick_prop = sum(sick_person) / n()) %>%
        pull(sick_prop)
    )

ggplot() +
    geom_line(aes(x = fa_vals, y = sick_prop), colour = 'red') +
    geom_line(aes(x = fa_vals, y = sick_2_prop)) +
    expand_limits(y = 0) +
    xlab('False Alarm Rate') +
    ylab('Proportion of True Positives')

```


# Curse of Dimensionality

The "no-one is average" principle

```{r curse_of_dimensionality, echo=TRUE}
dim_vals <- c(2:10, 20, 30, 50, 100, 500)

n_samples <- 10000
n_dim     <- max(dim_vals)

data_sample <- matrix(rnorm(n_dim * n_samples, 0, 1), n_samples, n_dim)

calc_norms <- function(i) apply(data_sample[, 1:i], 1, function(x) sqrt(sum(x * x)))

norms <- map(dim_vals, calc_norms)

plot_tbl <- tibble(dist = norms) %>%
    mutate(size = dim_vals) %>%
    unnest()

ggplot(plot_tbl) +
    geom_histogram(aes(x = dist), bins = 50) +
    facet_wrap(~size) +
    scale_y_continuous(labels = comma) +
    xlab("Distance") +
    ylab("Count")
```

Distance matrix gets large quickly, so just going to sample the average
distances.


```{r calculate_average_distance, echo=TRUE}
calc_avg_dist <- function(samp_points, n_iter = 100) {
    idx <- sample(1:n_samples, n_iter, replace = TRUE)

    dists <- samp_points[idx,] %>% dist %>% as.vector

    return(mean(dists))
}

mean_dists_tbl <- rerun(20, tibble(size = dim_vals) %>%
    mutate(dist = map_dbl(dim_vals
                         ,function(i) calc_avg_dist(data_sample[,1:i], 100)))
    ) %>%
    bind_rows()

ggplot(mean_dists_tbl) +
    geom_point(aes(x = size, y = dist)) +
    expand_limits(y = 0) +
    xlab("Dimensions") +
    ylab("Euclidean Data")
```

How much of a unit sphere is in the unit cube?

```{r unit_sphere_cube, echo=TRUE}
n_dim     <- 20
n_samples <- 1000000

data_unitcube <- matrix(runif(n_dim * n_samples, 0, 1), n_samples, n_dim)

calc_sphere_prop <- function(i) {
    data_mat <- data_unitcube[,1:i]

    dists <- apply(data_mat, 1, function(x) sqrt(sum(x * x)))

    return(length(dists[dists <= 1]) / length(dists))
}


sphere_prop <- map_dbl(2:n_dim, calc_sphere_prop)

ggplot() +
    geom_line(aes(x = 2:n_dim, y = sphere_prop)) +
    expand_limits(y = 0) +
    xlab("Dimension") +
    ylab("Proportion")
```


# Central Limit Theorem

## Normal Distribution

```{r show_clt_normal_data, echo=TRUE}
clt_count <- c(2, 3, 4, 5, 10, 20, 30, 50, 100, 1000)

n_samples <- 10000
n_dim     <- max(clt_count)

data_clt <- matrix(rnorm(n_dim * n_samples, 0, 1), n_samples, n_dim)

clt_norm_data_tbl <- tibble(vals = map(clt_count, function(i) rowMeans(data_clt[,1:i]))) %>%
    mutate(size     = clt_count
          ,size_str = factor(clt_count)
           ) %>%
    unnest()


ggplot(clt_norm_data_tbl) +
    geom_line(aes(x = vals, colour = size_str), stat = 'density') +
    xlab("Mean of Sample") +
    ylab("Probability Density")

clt_norm_data_tbl %>%
    group_by(size) %>%
    summarise(mean_val = mean(vals), sd_vals = sd(vals)) %>%
    ungroup %>%
    mutate(clt_sd = 1 / sqrt(size))
```


## Uniform Distribution

```{r show_clt_uniform_data, echo=TRUE}
clt_count <- c(2, 3, 4, 5, 10, 20, 30, 50, 100, 1000)

n_samples <- 10000
n_dim     <- max(clt_count)

data_clt <- matrix(runif(n_dim * n_samples, 0, 1), n_samples, n_dim)

clt_unif_data_tbl <- tibble(vals = map(clt_count, function(i) rowMeans(data_clt[,1:i]))) %>%
    mutate(size     = clt_count
          ,size_str = factor(clt_count)
           ) %>%
    unnest()


ggplot(clt_unif_data_tbl) +
    geom_line(aes(x = vals, colour = size_str), stat = 'density') +
    xlab("Mean of Sample") +
    ylab("Probability Density")

clt_unif_data_tbl %>%
    group_by(size) %>%
    summarise(mean_val = mean(vals), sd_vals = sd(vals)) %>%
    ungroup %>%
    mutate(clt_sd = 1 / sqrt(12 * size))
```


## Poisson Distribution

### Low Rate

```{r show_clt_lowpoisson_data, echo=TRUE}
clt_count <- c(2, 3, 4, 5, 10, 20, 30, 50, 100, 1000)

n_samples <- 10000
n_dim     <- max(clt_count)


data_clt <- matrix(rpois(n_dim * n_samples, 0.05), n_samples, n_dim)

clt_lowpois_data_tbl <- tibble(vals = map(clt_count, function(i) rowMeans(data_clt[,1:i]))) %>%
    mutate(size     = clt_count
          ,size_str = factor(clt_count)
           ) %>%
    unnest()


ggplot(clt_lowpois_data_tbl) +
    geom_histogram(aes(x = vals), bins = 50) +
    facet_wrap(~size, scales = 'free_x') +
    scale_y_continuous(labels = comma) +
    xlab("Mean of Sample") +
    ylab("Count")

clt_lowpois_data_tbl %>%
    group_by(size) %>%
    summarise(mean_val = mean(vals), sd_vals = sd(vals)) %>%
    ungroup %>%
    mutate(clt_sd = sqrt(0.05) / sqrt(size))
```


### Medium Rate

```{r show_clt_mediumpoisson_data, echo=TRUE}
clt_count <- c(2, 3, 4, 5, 10, 20, 30, 50, 100, 1000)

n_samples <- 10000
n_dim     <- max(clt_count)

count_rate <- 5


data_clt <- matrix(rpois(n_dim * n_samples, count_rate), n_samples, n_dim)

clt_lowpois_data_tbl <- tibble(vals = map(clt_count, function(i) rowMeans(data_clt[,1:i]))) %>%
    mutate(size     = clt_count
          ,size_str = factor(clt_count)
           ) %>%
    unnest()


ggplot(clt_lowpois_data_tbl) +
    geom_histogram(aes(x = vals), bins = 50) +
    facet_wrap(~size, scales = 'free_x') +
    scale_y_continuous(labels = comma) +
    xlab("Mean of Sample") +
    ylab("Count")

clt_lowpois_data_tbl %>%
    group_by(size) %>%
    summarise(mean_val = mean(vals), sd_vals = sd(vals)) %>%
    ungroup %>%
    mutate(clt_sd = sqrt(0.05) / sqrt(size))
```


### High Rate

```{r show_clt_highpoisson_data, echo=TRUE}
clt_count <- c(2, 3, 4, 5, 10, 20, 30, 50, 100, 1000)

n_samples <- 10000
n_dim     <- max(clt_count)

count_rate <- 100


data_clt <- matrix(rpois(n_dim * n_samples, count_rate), n_samples, n_dim)

clt_lowpois_data_tbl <- tibble(vals = map(clt_count, function(i) rowMeans(data_clt[,1:i]))) %>%
    mutate(size     = clt_count
          ,size_str = factor(clt_count)
           ) %>%
    unnest()


ggplot(clt_lowpois_data_tbl) +
    geom_histogram(aes(x = vals), bins = 50) +
    facet_wrap(~size, scales = 'free_x') +
    scale_y_continuous(labels = comma) +
    xlab("Mean of Sample") +
    ylab("Count")

clt_lowpois_data_tbl %>%
    group_by(size) %>%
    summarise(mean_val = mean(vals), sd_vals = sd(vals)) %>%
    ungroup %>%
    mutate(clt_sd = sqrt(0.05) / sqrt(size))
```


# Confidence Intervals

## Normal Distribution: N(0,1)

We take a sample of draws from $\mathcal{N}(0, 1)$. We want to use these
samples to inferences of the underlying population mean.

```{r create_sample_data, echo=TRUE}
calc_subsamp_stats <- function(i, data_matrix) {
    subsamp <- data_matrix[,1:i]
    
    mean <- rowMeans(subsamp)
    sd   <- rowSds(subsamp) / sqrt(i)
    
    return(tibble(mean = mean, sd = sd))
}

is_interval_inside <- function(x, mean, sd, mult) {
    (mean - mult * sd) < x & (mean + mult * sd) > x
}


clt_count <- c(2, 3, 4, 5, 10, 20, 30, 50, 100, 1000)

n_samples <- 100000
n_dim     <- max(clt_count)

data_confint <- matrix(rnorm(n_dim * n_samples, 0, 1), n_samples, n_dim)

confint_tbl <- tibble(size = clt_count) %>%
    mutate(data = map(size, calc_subsamp_stats, data_matrix = data_confint)) %>%
    unnest() %>%
    mutate(inside_1sd = is_interval_inside(0, mean, sd, 1)
          ,inside_2sd = is_interval_inside(0, mean, sd, 2)
           ) %>%
    group_by(size) %>%
    summarise(prop_1sd = sum(inside_1sd) / n()
             ,prop_2sd = sum(inside_2sd) / n()
              )

confint_tbl %>% print()
```


# p-Value


# Subsampling: With vs Without Replacement

---

Thank You!!!

\

mickcooney@gmail.com

\

https://github.com/kaybenleroll/stats_through_simulation
